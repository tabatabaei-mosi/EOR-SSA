from pathlib import Path
from typing import Dict, List, Tuple, Union

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from loguru import logger
from sklearn.metrics import confusion_matrix
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall

# Initialize the logger
logger.add("logfile.log")

# Function to log messages and print them


def log_and_print(message):
    logger.info(message)
    print(message)


def path_check(
    file_path: str,
    path_return: str = False
):
    """
    check the path, if it doesn't exist, create it.

    Arguments:
        file_path {str} -- the path should be checked.

    keyword Arguments:
        path_return {bool} -- if the path doesn't exist, return the path string (default: False)

    Returns:
        str -- the path file (if path_return is True)
    """

    # check if the path exist
    path_exist = Path(file_path).exists()

    # if the path doesn't exist, create it
    if not path_exist:
        Path(file_path).mkdir(parents=True)

    # return the created path
    if path_return:
        return file_path


def f1_accuracy(recall, precision):
    """
    f1 score for the model.

    Arguments:
        recall {array-like} -- recall score calculated by the model

        precision {array-like} -- precision score calculated by the model

    Returns:
       array-like -- calculated f1 score for the model
    """

    f1 = 2 * (precision * recall) / (precision + recall)

    return f1


def ann_metrics_history(
    xdata, ydata,
    n_epoch, weights_history, ann_model,
    digit=2,
):
    """
    By using ann weights history (generated by SSA), this function calculates the performance metrics for each epoch.
    The performance metrics include cross-entropy accuracy, recall, precision, and F1 score.
    The dictionary result will use to visualize the performance metrics history.

    Arguments:

        xdata: Input data (x_train).
        ydata: Target data (y_train).
        n_epoch: Number of epochs.
        weights_history: global best weights generated by SSA in each epoch. For each epoch, the weights are stored in a list
            which first element is the weights array and second element is the fitness value.
        ann_model: Artificial neural network model instance (i.e. hybrid model instance)

    Keyword Arguments:

        digit: Number of decimal places to round the performance metrics (default is 2).

    Returns
        A dictionary containing the history (list) of performance metrics.
        The keys are: 'cce_history', 'recall_history', 'precision_history', 'f1_history'
    """

    # Initialize the metrics dictionaries
    metrics_history = {
        "cce_history": [],
        "recall_history": [],
        "precision_history": [],
        "f1_history": []
    }

    # Initialize the metrics objects
    cce = CategoricalAccuracy()
    r = Recall()
    p = Precision()

    # Iterate over the epochs
    for i in range(n_epoch + 1):
        # Calculate the predictions for the current epoch
        yhat = ann_model.prediction(
            solution=weights_history[i][0], x_data=xdata)

        # Update the metrics objects with the current predictions
        cce.update_state(ydata, yhat)
        r.update_state(ydata, yhat)
        p.update_state(ydata, yhat)

        # Calculate and store the metrics for the current epoch
        metrics_history["cce_history"].append(
            round(cce.result().numpy() * 100, digit))
        metrics_history["recall_history"].append(
            round(r.result().numpy() * 100, digit))
        metrics_history["precision_history"].append(
            round(p.result().numpy() * 100, digit))

        # Calculate and store the F1 score for the current epoch
        f1_score = round(f1_accuracy(
            metrics_history["recall_history"][-1], metrics_history["precision_history"][-1]), digit)
        metrics_history["f1_history"].append(f1_score)

        # Reset the metrics objects for the next epoch
        cce.reset_states()
        r.reset_states()
        p.reset_states()

    return metrics_history


def ann_loss_history(
    xdata, ydata,
    n_epoch, weights_history, ann_model,
    digit=2,
):
    """
    By using ann weights history (generated by SSA), this function track the CCE loss for each epoch.
    The result will use to visualize the performance history.

    Arguments:
        xdata {np.ndarray} -- input data (x_train)

        ydata {np.ndarray} -- output data (y_train)

        n_epoch {int} -- number of epochs

        weights_history {np.ndarray} -- global best weights generated by SSA in each epoch. For each epoch, the weights are stored in a list
            which first element is the weights array and second element is the fitness value.

        ann_model {object} -- an instance of hybrid model (Combination of SSA and ANN)

    Keyword Arguments:
        digit {int} -- Number of decimal places to round the performance metrics (default is 2).

    Returns:
        list -- A list containing the CCE loss for each epoch.
    """

    # Initialize the loss history list
    loss_history = []

    # Iterate over the epochs
    for i in range(n_epoch + 1):

        # Calculate the predictions for the current epoch
        yhat = ann_model.prediction(
            solution=weights_history[i][0], x_data=xdata)

        # Calculate the CCE loss for the current epoch
        cce = CategoricalCrossentropy()
        loss = round(cce(ydata, yhat).numpy(), digit)

        # Store the loss for the current epoch
        loss_history.append(loss)

    return loss_history


def best_performance(history, score=False):
    """
    Analyze the history performance of a model (accuracy or loss) by finding the epoch with the best results.

    Arguments:
        history (list): A list of performance metrics (either loss or score) for each epoch.

    Keyword Arguments:
        score (bool, optional): Set to True if the history represents scores.
        Default is False, which assumes the history represents loss values.

    Returns:
        dict: A dictionary containing the best performance metric and the corresponding epoch index.
        The keys are: 'best_metric', 'best_epoch'
    """
    # Find the best performance metric (min_loss or max_score) and its index
    if score:
        best_metric, best_index = max((val, idx)
                                      for idx, val in enumerate(history))
    else:
        best_metric, best_index = min((val, idx)
                                      for idx, val in enumerate(history))

    # Round the best performance metric to 3 decimal places
    best_metric = round(best_metric, 3)

    # Return the results in a dictionary
    return {'best_metric': best_metric, 'best_epoch': best_index}


def decode_label(output_codes, re_group=False):
    """
    decode the label code (retruned by model or true outputs) to the true label (EOR method).
    The results of this function is used for plotting the confusion matrix.

    Arguments:
        output_codes {set} -- the label codes returned by the model (true or predicted output)

    Keyword Arguments:
        re_group {bool} -- If the data is re-grouped to contain only 3 EOR category to balance dataset (default: True)

    Returns:
        list -- the true label of the EOR methods (The real name of the EOR methods) 
    """

    # The list of true label of the EOR methods
    decode_ouputs = list()

    if re_group:
        # If re-grouped strategy is used
        true_label = ['Thermal methods',
                      'Gas miscible/im.', 'Chemical and others']

    else:
        # The whole list of EOR methods in dataset (sorted in a list according to the code assigned to each method)
        true_label = [
            'Steam', 'HW', 'Combustion', 'CO2 mis', 'CO2 immis',
            'H mis', 'H immis', 'N immi', 'Polymer',
            'AGM', 'Microbial', 'Cyclic steam', 'HC mis/Water', 'AGA', 'Steam-SAGD'
        ]

    # Decode the label code to the true label
    for code in output_codes:
        decode_ouputs.append(true_label[code])

    return decode_ouputs


def conf_matrix(
    xdata, ydata,
    n_epoch, ann_model,
    weights_history,
    history=False, Category=False
):
    """
    Compute confusion matrix for the given data and model over the history or just for the last epoch.
    Moreover, find the EOR labels used by model for visulization


    Arguments:
        xdata (array-like): Input data
        ydata (array-like): True output data
        n_epoch (int): Number of epochs
        ann_model (object): An instance of hybrid model (Combination of SSA and ANN)
        weights_history (array-like): global best weights generated by SSA in each epoch. For each epoch, the weights are stored in a list
            which first element is the weights array and second element is the fitness value.

    Keyword Arguments:
        history (bool, optional): If True, compute confusion matrix and EOR labels for each epoch, else just for the last epoch. Defaults to False.
        Category (bool, optional): If True, use re-grouped labels (3 Categories). Defaults to False (All the EOR labels).

    Returns:
        tuple: If history is True, return a tuple of two lists. 
        The first list contains the confusion matrix for each epoch, and the second list contains the EOR labels for each epoch.
        otherwise (history=False), return a tuple of two arrays (confusion matrix and EOR labels)
    """

    # Function to process predictions and true labels
    def process_predictions(y_hat, y_true):
        """
        process over the y_true and y_predict to find the real EOR label. 
        Additionally, calculate the confusion matrix

        Arguments:
            y_hat {array-like} -- y_hat (final result of model by argmax)
            y_true {array-like} -- y_true (true label of the data)

        Returns:
            tuple -- confusion matrix and the real EOR label 
        """
        # the set of unique label codes
        class_code_true = set(y_true)
        class_code_pre = set(y_hat)

        # combine the two sets to get the unique labels
        class_code = class_code_true | class_code_pre

        # decode the label code to the real EOR label
        class_label = decode_label(class_code, Category)

        # Calculate confusion matrix
        c_matrix = confusion_matrix(y_true, y_hat)

        return c_matrix, class_label

    if history:
        # Initialize a list to store confusion matrices
        conf_matrix_history = list()

        # Loop over epochs
        for i in range(n_epoch + 1):
            # Predict labels using the ANN model
            predicted_output = ann_model.prediction(
                solution=weights_history[i][0], x_data=xdata)

            # argmax to get the class index (round the output to an available category)
            predicted_class_indices = np.argmax(predicted_output, axis=1)
            true_output_indices = np.argmax(ydata, axis=1)

            # Process predictions and true labels to get confusion matrix and EOR labels
            c_matrix, class_label = process_predictions(
                predicted_class_indices, true_output_indices)

            # Append confusion matrix to the list
            conf_matrix_history.append(c_matrix)

        return conf_matrix_history, class_label

    else:
        # Predict labels using the ANN model
        predicted_output = ann_model.prediction(
            solution=weights_history[-1][0], x_data=xdata)

        # argmax to get the class index (round the output to an available category)
        predicted_class_indices = np.argmax(predicted_output, axis=1)
        true_output_indices = np.argmax(ydata, axis=1)

        # Process predictions and true labels to get confusion matrix and EOR labels
        c_matrix, class_label = process_predictions(
            predicted_class_indices, true_output_indices)

        return c_matrix, class_label


def history_process(
    xdata: np.ndarray,
    ydata: np.ndarray,
    n_epoch: int,
    weights_history: List[Tuple[np.array, ...]],
    ann_model: object,
    digit: int = 2,
    cross_entropy_loss: bool = False
) -> Dict[str, List[float]]:
    """
    By using ann weights history (generated by SSA), this function calculates the performance metrics for each epoch.

    Arguments:
        xdata {np.ndarray} -- Input data for training
        ydata {np.ndarray} -- Output data for training
        n_epoch {int} -- number of epochs
        weights_history {List[Tuple[np.array, ...]]} -- global best weights generated by SSA in each epoch.

    Keyword Arguments:
        digit {int} -- number of decimal accuracy (default: {2})
        cross_entropy_loss {bool} -- The Loss function method (default: {False})

    Returns:
        Dict[str, List[float]] -- A dictionary containing the history of performance metrics.
    """

    # Initialize the metric objects outside the loop
    cce = CategoricalAccuracy()
    r = Recall()
    p = Precision()

    metric_history = {
        "cce_history": [],
        "recall_history": [],
        "precision_history": [],
        "f1_history": [],
    }

    for i in range(n_epoch + 1):
        yhat = ann_model.prediction(
            solution=weights_history[i][0], x_data=xdata)

        if cross_entropy_loss:
            cce_metric = CategoricalCrossentropy()
            loss = round(cce_metric(ydata, yhat).numpy(), digit)
            metric_history["cce_history"].append(loss)

        # Update the metrics
        cce.update_state(ydata, yhat)
        r.update_state(ydata, yhat)
        p.update_state(ydata, yhat)

        # Calculate and round the metric scores
        cce_score = round(cce.result().numpy() * 100, digit)
        r_score = round(r.result().numpy() * 100, digit)
        p_score = round(p.result().numpy() * 100, digit)

        # Calculate F1 score (you might want to provide your implementation of f1_accuracy)
        f1_score = round(f1_accuracy(r_score, p_score), digit)

        # Append the scores to their respective lists
        metric_history["recall_history"].append(r_score)
        metric_history["precision_history"].append(p_score)
        metric_history["f1_history"].append(f1_score)

    return metric_history


def write_df(history, epoch, item):
    """
    Create a DataFrame from lists of training, testing, and validation data.

    Args:
        history (list): A list containing three lists: training, testing, and validation data.
        epoch (int): Number of epochs.
        item (str): The item you want to display in the DataFrame (e.g., 'Loss', 'Accuracy').

    Returns:
        pd.DataFrame: A DataFrame with columns like 'Item 0', 'Item 1', ..., 'Item epoch' and rows for 'Train', 'Test', and 'Validation'.
    """
    train_list, test_list, val_list = history[0], history[1], history[2]

    # Initialize a dictionary to store the data
    df_dic = {}

    # Create columns for each epoch and store data for 'Train', 'Test', and 'Validation'
    for i in range(epoch + 1):
        df_dic[f'{item} {i}'] = [train_list[i], test_list[i], val_list[i]]

    # Create the DataFrame with an index for 'Train', 'Test', and 'Validation'
    dataframe = pd.DataFrame(df_dic, index=['Train', 'Test', 'Validation'])

    return dataframe


def plot_metrics(
    train_metrics: List[float],
    test_metrics: List[float],
    train_cm: np.ndarray,
    test_cm: np.ndarray,
    epoch: int,
    labels: List[str],
):
    """
    Plot various metrics, confusion matrices, and performance summaries.

    Args:
        train_metrics (List[float]): Training metrics.
        test_metrics (List[float]): Test metrics.
        val_metrics (List[float]): Validation metrics.
        train_cm (np.ndarray): Training confusion matrix.
        test_cm (np.ndarray): Test confusion matrix.
        val_cm (np.ndarray): Validation confusion matrix.
        epoch (int): Current epoch.
        labels (List[str]): Class labels for the confusion matrices.
    """
    # Best performance and epoch for RMSE
    b_rmse_t, b_rmse_t_ep = best_performance(train_metrics)

    # Best performance and epoch for CCE
    b_cce_t, b_cce_t_ep = best_performance(test_metrics)

    # Plot RMSE metrics
    fig1, axs1 = plt.subplots(2, figsize=(10, 12))
    axs1[0].set_title(
        "Best Validation Performance %.3f at epoch %s \n\nBest Test Performance %.3f at epoch %r\n\n" % (
            b_rmse_v, b_rmse_v_ep, b_rmse_t, b_rmse_t_ep))
    axs1[0].plot(train_metrics, label='Train', color='blue')
    axs1[0].plot(test_metrics, label='Test', color='red')
    axs1[0].legend(frameon=False)
    axs1[0].set(xlabel=f'{epoch} epoch', ylabel='RMSE')
    axs1[1].semilogy(train_metrics, label='Train', color='blue')
    axs1[1].semilogy(test_metrics, label='Test', color='red')
    axs1[1].legend(frameon=False)
    axs1[1].set(xlabel=f'{epoch} epoch', ylabel='Log RMSE')
    plt.show()
    plt.savefig(f'Log_Results/{fig1}')

    # Plot CCE metrics
    fig2, axs2 = plt.subplots(2, figsize=(10, 12))
    axs2[0].set_title(
        "Best Validation Performance %.3f at epoch %s \n\nBest Test Performance %.3f at epoch %r\n\n" % (
            b_cce_v, b_cce_v_ep, b_cce_t, b_cce_t_ep))
    axs2[0].plot(train_metrics, label='Train', color='blue')
    axs2[0].plot(test_metrics, label='Test', color='red')
    axs2[0].legend(frameon=False)
    axs2[0].set(xlabel=f'{epoch} epoch',
                ylabel='Categorical Cross entropy - Loss')
    axs2[1].semilogy(train_metrics, label='Train', color='blue')
    axs2[1].semilogy(test_metrics, label='Test', color='red')
    axs2[1].legend(frameon=False)
    axs2[1].set(xlabel=f'{epoch} epoch',
                ylabel='Log Categorical Cross entropy - Loss')
    plt.show()
    plt.savefig(f'Log_Results/{fig2}')

    # Plot confusion matrices
    plot_confusion_matrix(
        train_cm, labels, f'Train Confusion Matrix at epoch = {epoch}')
    plot_confusion_matrix(
        test_cm, labels, f'Test Confusion Matrix at epoch = {epoch}')


def plot_confusion_matrix(cm: np.ndarray, labels: List[str], title: str):
    """
    Plot a confusion matrix.

    Args:
        cm (np.ndarray): Confusion matrix.
        labels (List[str]): Class labels for the confusion matrix.
        title (str): Title for the plot.
    """
    fig, ax = plt.subplots(figsize=(12, 12))
    group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
    group_percentages = ["{0:0.0%}".format(
        value) for value in cm.flatten() / np.sum(cm)]
    cm_labels = [f"{v1}\n{v2}\n" for v1, v2 in zip(
        group_counts, group_percentages)]
    n_classes = cm.shape[0]
    cm_labels = np.asarray(cm_labels).reshape(n_classes, n_classes)
    sns.heatmap(cm, annot=cm_labels, fmt='', cmap='Blues', ax=ax)
    ax.set_title(title)
    ax.xaxis.set_ticklabels(labels, rotation=45,
                            ha='right', rotation_mode='anchor')
    ax.yaxis.set_ticklabels(labels, rotation=45,
                            ha='right', rotation_mode='anchor')
    ax.set_xlabel('\nPredicted Category Class')
    ax.set_ylabel('True Category Class')
    plt.show()
